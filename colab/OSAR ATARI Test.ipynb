{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSAR Context Generator PoW test\n",
    "\n",
    "**Author:** [Konstantin Ustyuzhanin](https://www.facebook.com/konstantin.ustuzhanin/)<br>\n",
    "**Date created:** 2021/01/27<br>\n",
    "**Last modified:** 2021/02/10<br>\n",
    "**Description:** Implement Objective Stimuli Active Repeater (OSAR) Context Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing d:\\jorgen\\projects\\python\\phdsub\\towerproject\\osar-keras\n",
      "Requirement already satisfied: tensorflow>=2.3.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from OSAR==0.1.5) (2.4.1)\n",
      "Requirement already satisfied: gin-config>=0.1.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from OSAR==0.1.5) (0.4.0)\n",
      "Requirement already satisfied: tf_agents>=0.6.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from OSAR==0.1.5) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata~=1.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from OSAR==0.1.5) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from importlib-metadata~=1.0->OSAR==0.1.5) (3.4.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.3.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (2.10.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.11.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.12)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.32.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (2.4.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (3.14.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.26.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.26.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.12.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (0.12.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (1.6.0)\n",
      "Requirement already satisfied: gym>=0.17.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (0.18.0)\n",
      "Requirement already satisfied: pillow>=7.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (7.2.0)\n",
      "Requirement already satisfied: dm-tree in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow-probability>=0.12.1->tf_agents>=0.6.0->OSAR==0.1.5) (0.1.5)\n",
      "Requirement already satisfied: decorator in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow-probability>=0.12.1->tf_agents>=0.6.0->OSAR==0.1.5) (4.4.2)\n",
      "Building wheels for collected packages: OSAR\n",
      "  Building wheel for OSAR (setup.py): started\n",
      "  Building wheel for OSAR (setup.py): finished with status 'done'\n",
      "  Created wheel for OSAR: filename=OSAR-0.1.5-py3-none-any.whl size=60214 sha256=70dd206f3d6e22e7479a493772f718008e9828b2dc3739f8f909bcbba3a7cec4\n",
      "  Stored in directory: c:\\users\\jorgen\\appdata\\local\\pip\\cache\\wheels\\df\\60\\40\\81c908253910a541e172b609cc54a7a5400e85a6fba88b292e\n",
      "Successfully built OSAR\n",
      "Installing collected packages: OSAR\n",
      "  Attempting uninstall: OSAR\n",
      "    Found existing installation: OSAR 0.1.5\n",
      "    Uninstalling OSAR-0.1.5:\n",
      "      Successfully uninstalled OSAR-0.1.5\n",
      "Successfully installed OSAR-0.1.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !pip install -q gym\n",
    "# Install additional packages for visualization\n",
    "if sys.platform != 'win32':\n",
    "    !sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "# !pip install pyvirtualdisplay #> /dev/null 2>&1\n",
    "# !pip install -q imageio == 2.4.0\n",
    "# !pip install PILLOW\n",
    "# !pip install pyglet\n",
    "# !pip install -q git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
    "# !pip install git+https://github.com/ustyuzhaninky/OSAR-keras\n",
    "!pip install ../../OSAR-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import numpy as np\n",
    "from tf_agents import agents\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OSAR import OSARNetwork, DQNRewardedAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10 # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 10000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 1  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 1  # @param {type:\"integer\"}\n",
    "memory_len = 3 # @param {type: \"integer\"}\n",
    "n_turns = 3 # @param {type: \"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
    "\n",
    "Load the CartPole environment from the OpenAI Gym suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Alien-ram-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIHYVBkuvPNw"
   },
   "source": [
    "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAAEsklEQVR4nO3dMXLTQBiGYYXhGOloaFKQyeQEuYM7V5QUKTlCSg5ARccNUnACTwa3aeg4CIVmPB4SKZJ2/9Xuq/epMoaIDR8flle/5a6TJElSdhdDv3B1/fjq4x9+P4UtRm/7c30z6/e/C1qHKmHAcAYMZ8BwBgxnwHAGDPc+5ZvnvibTMil7DzYYzoDhDBhucwEf989rL6GozQW8NZkD3h0u8x7wJL15x/1zf5DTF1uQ9DKpLZ9+fOy67rh/7r/YiMwN/nn799XH05s9lMp2urjM5p6DN1XfrljANnst2AY/dF/WXkIVVg54qNkZjvzjV9CR24JtsHoGDGfAcCEbHblmp4euN681mx29nojr6zYYLnODd4fL/06M5/6rHGrDbn/36olx9FTJ3Hbm+nlzscFwhfaiMxzZ17WL2GC48MuF0c8xtb3bsbb12GC48AZHnOXmOlPNtbaU43gWrSQGDGfAcAYMZ8BwzcxF7/Z3QUdms8FwzcxFD+1F2+xxNhiu+blomz3OBsM5Fw1ng+EMGM6A4VZ7A3jEddBc13drXttcNhgufC56iponImpe2xQ2GK6ZvWgtY4PhCp1FT39izvW8NfReprmi35sUzQbDhTS4hnnj8/rOPU7JdwhGN94Gw20u4N3+blOXijcXcLexK4lbDNgGw9ng5ZqYi7bB4mhmL9rpyWVsMNzKc9EZjmyzR9lgOGzAm3otNAIbsHoGDLehz03qbe2/7qSAp1wHjbjeWdv9olN4nywlCfkE8Ih/la3cLzpFxN+nDYYrFHDcXrTG2WC48LPo3vhedMpzTA3v/zlX289ig+Euhn7h6vqx5DoUxAbDGTCcAcMZMJwBwzUzF308fA86MpsNhmtmLvrT7edXH7fZ42wwXPP3i7bZ42wwnHvRcDYYzoDhDBhu9kRHrju/lbwn89zPD65hbXOPM8QGw1UxF13zPZlLrs25aM3WzFy0lrHBcM3PRQ85Hr4PbVPPUnLu2rlozRZyFh39vVOk1LfkXdun/FmeRWuQAcMZMJwBwxkwnHPRcDYYzrloOBsM51w0nA2GWznguDvBZ7mUBGCD4QwYzoDhQm4InmsyIXpuuYapxynHT2GD4TI3eHe4/O/EONenaQ/NWNV2v+iSnx4+hQ2GK7QXnc7XtcvYYLjwj9WJfo4h3S86gg2GC2lw9DvySs4tz1Xb2mwwnAHDGTCcAcMZMJxz0XA2GM65aDgbDOdcNJwNhnMuGs4GwxkwnAHDrTYXnev+UNF3vC05g+1ctGYrOhe9VqtqPo5z0UrSzFy0lrHBcCvPRb98zp7+vbnW0O5xprDBcCvPRb+sb8RrwZSz31zrWWte2gbDGTCcAcMZMJwBwzkXDWeD4ZyLhrPBcNi5aPVsMBx2Llo9GwxnwHAGDFfoE8DrR/pZztlgOAOGCx+6i3bz7evp66f7hxVXUqe2G9yn+3T/0Ed7HrZ6bQesNxkwnAHDGTBc2wGfzq1OZ1trr6g6zb9MMtRxbTdYbzJgOAOGSw34dIIz8XEVlhrw0B6he4eVyNDg0xfncQ49rsLyNLg72/Eff1yF5WnwywjdeahE6kbHUIRGK0mSJEmSJEmSJEmStNQ/7LzUVf4jTwoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x23920099100>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9_lskPOey18"
   },
   "source": [
    "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n",
    "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(128,), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(128,), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=17)\n"
     ]
    }
   ],
   "source": [
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JSc9GviWUBK"
   },
   "source": [
    "Usually two environments are instantiated: one for training and one for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "N7brXNIGWXjC"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "#         discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "                   (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int,\n",
    "    ) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards, x_values = run_episode(\n",
    "            initial_state, model, max_steps_per_episode) \n",
    "\n",
    "        # Calculate expected returns\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "        # Convert training data to appropriate TF tensor shapes\n",
    "        action_probs, values, returns = [\n",
    "#             tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "            tf.cast(x, tf.float32) for x in [action_probs, values, returns]] \n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "        # Compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the model's parameters\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    episode_x = tf.math.reduce_mean(x_values)\n",
    "    episode_loss = tf.math.reduce_mean(loss)\n",
    "    episode_actions = tf.math.reduce_mean(action_probs)\n",
    "    episode_values = tf.math.reduce_sum(values)\n",
    "\n",
    "    return episode_reward, [episode_reward, episode_x, episode_loss, episode_actions, episode_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "    \n",
    "    time_step = env.step(action)\n",
    "    state, reward, done = time_step.observation, time_step.reward, 0\n",
    "    reward = tf.cast(reward, tf.float32)\n",
    "    return (state.astype(np.uint8), \n",
    "            np.array(reward, np.float32), \n",
    "            np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(env_step, [action], \n",
    "                             [tf.uint8, tf.float32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int,\n",
    "    initial_reward: tf.Tensor = tf.zeros((1,1), dtype=tf.float32), \n",
    ") -> List[tf.Tensor]:\n",
    "    \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    x_values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    reward = initial_reward\n",
    "        \n",
    "    for t in tf.range(max_steps):\n",
    "#         tf.autograph.experimental.set_loop_options(\n",
    "#             shape_invariants=[(reward, tf.TensorShape([1,]))]\n",
    "#             )\n",
    "        \n",
    "        if tf.rank(state) < 2:\n",
    "            state = tf.expand_dims(state, axis=0)\n",
    "            reward = tf.expand_dims(tf.expand_dims(reward, axis=-1), axis=-1)\n",
    "        \n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state, reward)\n",
    "        \n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, env.action_space.n-1)[0, 0]\n",
    "#         action = tf.argmax(action_logits_t, axis=-1)\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "        last_action = action_probs_t\n",
    "        \n",
    "        # store last_action values\n",
    "        x_values = x_values.write(t, last_action)\n",
    "        \n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, tf.reduce_max(action_probs_t, axis=-1))\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, episode_reward, done = tf_env_step(action)\n",
    "#         state.set_shape(initial_state_shape)\n",
    "\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, episode_reward)\n",
    "        reward = episode_reward\n",
    "        \n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    x_values = x_values.stack()\n",
    "\n",
    "    return action_probs, values, rewards, x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def run(model:tf.keras.Model,\n",
    "        env,\n",
    "        inits:int,\n",
    "        max_episodes:int,\n",
    "        max_steps_per_episode:int,\n",
    "        reward_threshold:float,\n",
    "        gamma:int,\n",
    "        ) -> List[tf.Tensor]:\n",
    "    \n",
    "    running_reward = 0\n",
    "    train_history = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    \n",
    "    with tqdm.trange(max_episodes) as t:\n",
    "        for i in t:\n",
    "            initial_state = env.reset().observation\n",
    "            episode_reward, hist = model.train_step(\n",
    "                initial_state, optimizer, gamma,\n",
    "                max_steps_per_episode)\n",
    "            episode_reward = tf.cast(episode_reward, tf.float32)\n",
    "            hist = tf.concat([\n",
    "                tf.cast(tf.expand_dims(x, 0), tf.float32) for x in hist], axis=0)\n",
    "            train_history = train_history.write(i, hist)\n",
    "            \n",
    "            running_reward = episode_reward*0.01 + running_reward*.99\n",
    "\n",
    "            t.set_description(f'Episode {i}')\n",
    "            if hasattr(episode_reward, 'numpy'):\n",
    "                episode_reward = episode_reward.numpy()\n",
    "                running_reward = running_reward.numpy()\n",
    "            else:\n",
    "                episode_reward = episode_reward\n",
    "                running_reward = running_reward\n",
    "            t.set_postfix(\n",
    "                episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "#             if i % 10 == 0:\n",
    "#                 print(f'Episode {i}: average reward: {running_reward:.2f}')\n",
    "            if tf.get_static_value(running_reward) > reward_threshold:  \n",
    "                break\n",
    "    \n",
    "    train_history = train_history.stack()\n",
    "    print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (10,)\n",
    "\n",
    "q_net = OSARNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    batch_size,\n",
    "    memory_len,\n",
    "    n_turns,\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    conv_type='1d',\n",
    "    )\n",
    "\n",
    "action_spec = tf.nest.flatten(env.action_spec())[0]\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "class ModelWrapper(tf.keras.models.Model):\n",
    "    def __init__(self, network, fc_layer_params):\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.network_state = ()\n",
    "        self.fc_layer_params = fc_layer_params\n",
    "#     @tf.function\n",
    "    def train_step(self, initial_state: tf.Tensor, \n",
    "            optimizer: tf.keras.optimizers.Optimizer, \n",
    "            gamma: float, \n",
    "            max_steps_per_episode: int, dtype=tf.float32) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "        return train_step(\n",
    "               initial_state, self, optimizer, gamma,\n",
    "               max_steps_per_episode)\n",
    "    @tf.function\n",
    "    def call(self, inputs: tf.Tensor, reward:tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        value, critic, self.network_state = self.network(inputs, reward=reward, )#network_state=self.network_state)\n",
    "        return value, critic\n",
    "    \n",
    "osar = ModelWrapper(q_net, fc_layer_params)\n",
    "osar.compile(loss=tf.keras.losses.Huber,\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "             run_eagerly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "num_hidden = 10\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 300#195.0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]C:\\Users\\Jorgen\\AppData\\Local\\Temp\\tmpl3bp49vd.py:14: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  retval_ = ag__.and_((lambda : (ag__.ld(state) is not None)), (lambda : ag__.and_((lambda : (ag__.ld(state) is not ())), (lambda : (ag__.ld(state) is not [])))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_full_matrix.py:150: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "WARNING:tensorflow:From C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_kronecker.py:224: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not call `graph_parents`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 99:  10%|██▏                   | 100/1000 [1:46:48<19:41:37, 78.77s/it, episode_reward=140, running_reward=118]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# osar = get_OSAR(num_hidden=num_hidden,)\n",
    "osar\n",
    "with tf.device('/device:GPU:0'):\n",
    "    osar_hist = run(osar,\n",
    "                    train_env,\n",
    "                    num_hidden,\n",
    "                    max_episodes,\n",
    "                    max_steps_per_episode,\n",
    "                    reward_threshold,\n",
    "                    gamma,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['episode_reward', 'episode_x', 'episode_loss', 'episode_actions', 'episode_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_running_reward(rewards):\n",
    "    running_reward = [0]\n",
    "    for r in rewards:\n",
    "        running_reward.append(r*0.01 + running_reward[-1]*.99)\n",
    "    return running_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True, figsize=(10, 10))\n",
    "\n",
    "ax[0, 0].plot(get_running_reward(baseline_hist[:, 0]), '--', alpha=0.7, label='baseline')\n",
    "ax[0, 0].plot(get_running_reward(osar_hist[:, 0]), label='osar')\n",
    "ax[0, 0].plot(get_running_reward(gru_hist[:, 0]), alpha=0.4, label='GRU')\n",
    "ax[0, 0].set_title('Накопленная награда')\n",
    "ax[0, 0].legend()\n",
    "ax[1, 0].plot(baseline_hist[:, 2], '--', alpha=0.7, label='baseline')\n",
    "ax[1, 0].plot(osar_hist[:, 2], label='osar')\n",
    "ax[1, 0].plot(gru_hist[:, 2], alpha=0.4, label='gru')\n",
    "ax[1, 0].set_title('Функция потерь')\n",
    "ax[1, 0].legend()\n",
    "ax[0, 1].plot(get_running_reward(baseline_hist[:, 4]), '--', alpha=0.7, label='Критик baseline')\n",
    "ax[0, 1].plot(get_running_reward(osar_hist[:, 4]), label='Критик osar')\n",
    "ax[0, 1].plot(get_running_reward(baseline_hist[:, 0]),'--', alpha=0.7, label='Награда baseline')\n",
    "ax[0, 1].plot(get_running_reward(osar_hist[:, 0]), label='Награда osar')\n",
    "ax[0, 1].plot(get_running_reward(gru_hist[:, 0]), alpha=0.4, label='Награда GRU')\n",
    "ax[0, 1].plot(get_running_reward(gru_hist[:, 4]), alpha=0.4, label='Критик GRU')\n",
    "ax[0, 1].set_title('Реальная награда и ожидаемая награды Критика')\n",
    "ax[0, 1].legend()\n",
    "ax[1, 1].plot(baseline_hist[:, 3], '--', alpha=0.7, label='baseline')\n",
    "ax[1, 1].plot(osar_hist[:, 3], label='osar')\n",
    "ax[1, 1].plot(gru_hist[:, 3], alpha=0.4, label='GRU')\n",
    "ax[1, 1].set_title('Функция уверенности в выбранном действии')\n",
    "ax[1, 1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(gru_hist)), baseline_hist[:len(gru_hist), 0], alpha=0.7, label='baseline')\n",
    "plt.bar(range(len(gru_hist)), gru_hist[:, 0], alpha=0.4,\n",
    "        bottom=baseline_hist[:len(gru_hist), 0], label='GRU')\n",
    "plt.bar(range(len(gru_hist)), osar_hist[:len(gru_hist), 0], alpha=0.7, label='osar',\n",
    "       bottom=baseline_hist[:len(gru_hist), 0]+osar_hist[:len(gru_hist), 0])\n",
    "plt.title('Награда за эпизод игры')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "import sys\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "from pyvirtualdisplay import Display\n",
    "# import tensorflow_docs.vis.embed as embed\n",
    "\n",
    "if sys.platform != 'win32':\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "\n",
    "\n",
    "def render_episode(env, model: tf.keras.Model, max_steps: int, m_type:int=1, inits:int=num_hidden,): \n",
    "    screen = env.render(mode='rgb_array')\n",
    "    im = Image.fromarray(screen)\n",
    "\n",
    "    images = [im]\n",
    "    \n",
    "    reward, loop_var = tf.zeros((1, 1), tf.float32), tf.zeros((1, inits), tf.float32)\n",
    "\n",
    "    state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    for i in range(1, max_steps + 1):\n",
    "        \n",
    "        if m_type == 1: \n",
    "            action_probs, _, loop_var = model(state)\n",
    "            action = tf.random.categorical(action_probs, env.action_space.n)[0, 0]\n",
    "        else:\n",
    "            action_probs, _, loop_var = model([state, reward, tf.nn.softmax(loop_var)])\n",
    "            action = tf.random.categorical(action_probs, env.action_space.n)[0, 0]\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = tf.constant(state, dtype=tf.float32)\n",
    "        reward = tf.constant(reward, dtype=tf.float32)\n",
    "\n",
    "        # Render screen every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            screen = env.render(mode='rgb_array')\n",
    "            images.append(Image.fromarray(screen))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untrained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "buggy_baseline = get_baseline()\n",
    "run(buggy_baseline, 10, 100, max_steps_per_episode, reward_threshold, gamma, m_type=1)\n",
    "images = render_episode(env, buggy_baseline, max_steps_per_episode)\n",
    "image_untrained_baseline_file = '../images/alien-baseline-untrained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_untrained_baseline_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_untrained_baseline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "buggy_gru = get_GRU(num_hidden=10,)\n",
    "run(buggy_gru, 10, 100, max_steps_per_episode, reward_threshold, gamma, m_type=1)\n",
    "images = render_episode(env, buggy_gru, max_steps_per_episode, m_type=1)\n",
    "image_untrained_gru_file = '../images/alien-gru-untrained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_untrained_gru_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_untrained_osar_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSAR (Context Genrator + GRU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "buggy_osar = get_OSAR(num_hidden=10,)\n",
    "run(buggy_osar, 10, 100, max_steps_per_episode, reward_threshold, gamma, m_type=2)\n",
    "images = render_episode(env, buggy_osar, max_steps_per_episode, m_type=2)\n",
    "image_untrained_osar_file = '../images/alien-osar-untrained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_untrained_osar_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_untrained_osar_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "images = render_episode(env, baseline, max_steps_per_episode)\n",
    "image_trained_baseline_file = '../images/alien-baseline-trained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_trained_baseline_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_trained_baseline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "images = render_episode(env, gru, max_steps_per_episode)\n",
    "image_trained_gru_file = '../images/alien-gru-trained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_trained_gru_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_trained_baseline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSAR (Context Genrator + GRU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "images = render_episode(env, osar, max_steps_per_episode, m_type=2)\n",
    "image_trained_osar_file = '../images/alien-osar-trained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_trained_osar_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_trained_osar_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
