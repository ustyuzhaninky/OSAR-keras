{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSAR Context Generator PoW test\n",
    "\n",
    "**Author:** [Konstantin Ustyuzhanin](https://www.facebook.com/konstantin.ustuzhanin/)<br>\n",
    "**Date created:** 2021/01/27<br>\n",
    "**Last modified:** 2021/02/10<br>\n",
    "**Description:** Implement Objective Stimuli Active Repeater (OSAR) Context Generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing d:\\jorgen\\projects\\python\\phdsub\\towerproject\\osar-keras\n",
      "Requirement already satisfied: tensorflow>=2.3.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from OSAR==0.1.5) (2.4.1)\n",
      "Requirement already satisfied: gin-config>=0.1.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from OSAR==0.1.5) (0.4.0)\n",
      "Requirement already satisfied: tf_agents>=0.6.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from OSAR==0.1.5) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata~=1.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from OSAR==0.1.5) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from importlib-metadata~=1.0->OSAR==0.1.5) (3.4.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.12)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.19.5)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.11.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (3.14.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.3.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.12.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (2.4.1)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.15.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\jorgen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow>=2.3.0->OSAR==0.1.5) (1.6.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (3.3.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.26.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (0.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->OSAR==0.1.5) (3.1.0)\n",
      "Requirement already satisfied: gym>=0.17.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (1.6.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.12.1 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (0.12.1)\n",
      "Requirement already satisfied: pillow>=7.0.0 in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tf_agents>=0.6.0->OSAR==0.1.5) (7.2.0)\n",
      "Requirement already satisfied: decorator in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow-probability>=0.12.1->tf_agents>=0.6.0->OSAR==0.1.5) (4.4.2)\n",
      "Requirement already satisfied: dm-tree in c:\\anaconda3\\envs\\osar\\lib\\site-packages (from tensorflow-probability>=0.12.1->tf_agents>=0.6.0->OSAR==0.1.5) (0.1.5)\n",
      "Building wheels for collected packages: OSAR\n",
      "  Building wheel for OSAR (setup.py): started\n",
      "  Building wheel for OSAR (setup.py): finished with status 'done'\n",
      "  Created wheel for OSAR: filename=OSAR-0.1.5-py3-none-any.whl size=60729 sha256=fddc1b6ae4c711b8bf2903d4cb8f7732adf061278c2373e1538ceea7d70bd31f\n",
      "  Stored in directory: c:\\users\\jorgen\\appdata\\local\\pip\\cache\\wheels\\df\\60\\40\\81c908253910a541e172b609cc54a7a5400e85a6fba88b292e\n",
      "Successfully built OSAR\n",
      "Installing collected packages: OSAR\n",
      "  Attempting uninstall: OSAR\n",
      "    Found existing installation: OSAR 0.1.5\n",
      "    Uninstalling OSAR-0.1.5:\n",
      "      Successfully uninstalled OSAR-0.1.5\n",
      "Successfully installed OSAR-0.1.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !pip install -q gym\n",
    "# Install additional packages for visualization\n",
    "if sys.platform != 'win32':\n",
    "    !sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "# !pip install pyvirtualdisplay #> /dev/null 2>&1\n",
    "# !pip install -q imageio == 2.4.0\n",
    "# !pip install PILLOW\n",
    "# !pip install pyglet\n",
    "# !pip install -q git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
    "# !pip install git+https://github.com/ustyuzhaninky/OSAR-keras\n",
    "!pip install ../../OSAR-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import numpy as np\n",
    "from tf_agents import agents\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "# Configuration parameters for the whole setup\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OSAR import OSARNetwork, DQNRewardedAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 10 # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 10000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 1  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 1  # @param {type:\"integer\"}\n",
    "memory_len = 10 # @param {type: \"integer\"}\n",
    "n_turns = 5 # @param {type: \"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
    "\n",
    "Load the CartPole environment from the OpenAI Gym suite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Alien-ram-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIHYVBkuvPNw"
   },
   "source": [
    "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAAEsklEQVR4nO3dMXLTQBiGYYXhGOloaFKQyeQEuYM7V5QUKTlCSg5ARccNUnACTwa3aeg4CIVmPB4SKZJ2/9Xuq/epMoaIDR8flle/5a6TJElSdhdDv3B1/fjq4x9+P4UtRm/7c30z6/e/C1qHKmHAcAYMZ8BwBgxnwHAGDPc+5ZvnvibTMil7DzYYzoDhDBhucwEf989rL6GozQW8NZkD3h0u8x7wJL15x/1zf5DTF1uQ9DKpLZ9+fOy67rh/7r/YiMwN/nn799XH05s9lMp2urjM5p6DN1XfrljANnst2AY/dF/WXkIVVg54qNkZjvzjV9CR24JtsHoGDGfAcCEbHblmp4euN681mx29nojr6zYYLnODd4fL/06M5/6rHGrDbn/36olx9FTJ3Hbm+nlzscFwhfaiMxzZ17WL2GC48MuF0c8xtb3bsbb12GC48AZHnOXmOlPNtbaU43gWrSQGDGfAcAYMZ8BwzcxF7/Z3QUdms8FwzcxFD+1F2+xxNhiu+blomz3OBsM5Fw1ng+EMGM6A4VZ7A3jEddBc13drXttcNhgufC56iponImpe2xQ2GK6ZvWgtY4PhCp1FT39izvW8NfReprmi35sUzQbDhTS4hnnj8/rOPU7JdwhGN94Gw20u4N3+blOXijcXcLexK4lbDNgGw9ng5ZqYi7bB4mhmL9rpyWVsMNzKc9EZjmyzR9lgOGzAm3otNAIbsHoGDLehz03qbe2/7qSAp1wHjbjeWdv9olN4nywlCfkE8Ih/la3cLzpFxN+nDYYrFHDcXrTG2WC48LPo3vhedMpzTA3v/zlX289ig+Euhn7h6vqx5DoUxAbDGTCcAcMZMJwBwzUzF308fA86MpsNhmtmLvrT7edXH7fZ42wwXPP3i7bZ42wwnHvRcDYYzoDhDBhu9kRHrju/lbwn89zPD65hbXOPM8QGw1UxF13zPZlLrs25aM3WzFy0lrHBcM3PRQ85Hr4PbVPPUnLu2rlozRZyFh39vVOk1LfkXdun/FmeRWuQAcMZMJwBwxkwnHPRcDYYzrloOBsM51w0nA2GWznguDvBZ7mUBGCD4QwYzoDhQm4InmsyIXpuuYapxynHT2GD4TI3eHe4/O/EONenaQ/NWNV2v+iSnx4+hQ2GK7QXnc7XtcvYYLjwj9WJfo4h3S86gg2GC2lw9DvySs4tz1Xb2mwwnAHDGTCcAcMZMJxz0XA2GM65aDgbDOdcNJwNhnMuGs4GwxkwnAHDrTYXnev+UNF3vC05g+1ctGYrOhe9VqtqPo5z0UrSzFy0lrHBcCvPRb98zp7+vbnW0O5xprDBcCvPRb+sb8RrwZSz31zrWWte2gbDGTCcAcMZMJwBwzkXDWeD4ZyLhrPBcNi5aPVsMBx2Llo9GwxnwHAGDFfoE8DrR/pZztlgOAOGCx+6i3bz7evp66f7hxVXUqe2G9yn+3T/0Ed7HrZ6bQesNxkwnAHDGTBc2wGfzq1OZ1trr6g6zb9MMtRxbTdYbzJgOAOGSw34dIIz8XEVlhrw0B6he4eVyNDg0xfncQ49rsLyNLg72/Eff1yF5WnwywjdeahE6kbHUIRGK0mSJEmSJEmSJEmStNQ/7LzUVf4jTwoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210 at 0x2682C63E940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9_lskPOey18"
   },
   "source": [
    "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n",
    "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(128,), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(128,), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=17)\n"
     ]
    }
   ],
   "source": [
    "print('Action Spec:')\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JSc9GviWUBK"
   },
   "source": [
    "Usually two environments are instantiated: one for training and one for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "N7brXNIGWXjC"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "#         discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "                   (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int,\n",
    "    ) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards, x_values = run_episode(\n",
    "            initial_state, model, max_steps_per_episode) \n",
    "\n",
    "        # Calculate expected returns\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "        # Convert training data to appropriate TF tensor shapes\n",
    "        action_probs, values, returns = [\n",
    "#             tf.expand_dims(x, 1) for x in [action_probs, values, returns]]\n",
    "            tf.cast(x, tf.float32) for x in [action_probs, values, returns]] \n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "        # Compute the gradients from the loss\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        # Apply the gradients to the model's parameters\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    episode_x = tf.math.reduce_mean(x_values)\n",
    "    episode_loss = tf.math.reduce_mean(loss)\n",
    "    episode_actions = tf.math.reduce_mean(action_probs)\n",
    "    episode_values = tf.math.reduce_sum(values)\n",
    "\n",
    "    return episode_reward, [episode_reward, episode_x, episode_loss, episode_actions, episode_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "    \n",
    "    time_step = env.step(action)\n",
    "    state, reward, done = time_step.observation, time_step.reward, 0\n",
    "    reward = tf.cast(reward, tf.float32)\n",
    "    return (state.astype(np.uint8), \n",
    "            np.array(reward, np.float32), \n",
    "            np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(env_step, [action], \n",
    "                             [tf.uint8, tf.float32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    max_steps: int,\n",
    "    initial_reward: tf.Tensor = tf.zeros((1,), dtype=tf.float32), \n",
    ") -> List[tf.Tensor]:\n",
    "    \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    x_values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    reward = initial_reward\n",
    "        \n",
    "    for t in tf.range(max_steps):\n",
    "        tf.autograph.experimental.set_loop_options(\n",
    "            shape_invariants=[(reward, tf.TensorShape([1,]))]\n",
    "            )\n",
    "        \n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state, reward)\n",
    "        \n",
    "        # Sample next action from the action probability distribution\n",
    "#         action = tf.random.categorical(action_logits_t, env.action_space.n-1)[0, 0]\n",
    "        action = tf.argmax(action_logits_t, axis=-1)\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "        last_action = action_probs_t\n",
    "        \n",
    "        # store last_action values\n",
    "        x_values = x_values.write(t, last_action)\n",
    "        \n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, tf.reduce_max(action_probs_t, axis=-1))\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, episode_reward, done = tf_env_step(action)\n",
    "#         state.set_shape(initial_state_shape)\n",
    "\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, episode_reward)\n",
    "        reward = episode_reward\n",
    "\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    x_values = x_values.stack()\n",
    "\n",
    "    return action_probs, values, rewards, x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def run(model:tf.keras.Model,\n",
    "        env,\n",
    "        inits:int,\n",
    "        max_episodes:int,\n",
    "        max_steps_per_episode:int,\n",
    "        reward_threshold:float,\n",
    "        gamma:int,\n",
    "        ) -> List[tf.Tensor]:\n",
    "    \n",
    "    running_reward = 0\n",
    "    train_history = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    \n",
    "    with tqdm.trange(max_episodes) as t:\n",
    "        for i in t:\n",
    "            initial_state = env.reset().observation\n",
    "            episode_reward, hist = model.train_step(\n",
    "                initial_state, optimizer, gamma,\n",
    "                max_steps_per_episode)\n",
    "            episode_reward = tf.cast(episode_reward, tf.float32)\n",
    "            hist = tf.concat([\n",
    "                tf.cast(tf.expand_dims(x, 0), tf.float32) for x in hist], axis=0)\n",
    "            train_history = train_history.write(i, hist)\n",
    "            \n",
    "            running_reward = episode_reward*0.01 + running_reward*.99\n",
    "\n",
    "            t.set_description(f'Episode {i}')\n",
    "            if hasattr(episode_reward, 'numpy'):\n",
    "                episode_reward = episode_reward.numpy()\n",
    "                running_reward = running_reward.numpy()\n",
    "            else:\n",
    "                episode_reward = episode_reward\n",
    "                running_reward = running_reward\n",
    "            t.set_postfix(\n",
    "                episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "#             if i % 10 == 0:\n",
    "#                 print(f'Episode {i}: average reward: {running_reward:.2f}')\n",
    "            if tf.get_static_value(running_reward) > reward_threshold:  \n",
    "                break\n",
    "    \n",
    "    train_history = train_history.stack()\n",
    "    print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (10,)\n",
    "\n",
    "q_net = OSARNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    batch_size,\n",
    "    memory_len,\n",
    "    n_turns,\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    conv_type='1d',\n",
    "    )\n",
    "\n",
    "action_spec = tf.nest.flatten(env.action_spec())[0]\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "class ModelWrapper(tf.keras.models.Model):\n",
    "    def __init__(self, network, fc_layer_params):\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.network_state = ()\n",
    "        self.fc_layer_params = fc_layer_params\n",
    "    @tf.function\n",
    "    def train_step(self, initial_state: tf.Tensor, \n",
    "            optimizer: tf.keras.optimizers.Optimizer, \n",
    "            gamma: float, \n",
    "            max_steps_per_episode: int, dtype=tf.float32) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
    "        return train_step(\n",
    "               initial_state, self, optimizer, gamma,\n",
    "               max_steps_per_episode)\n",
    "    @tf.function\n",
    "    def call(self, inputs: tf.Tensor, reward:tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        value, critic, self.network_state = self.network(inputs, reward=reward, )#network_state=self.network_state)\n",
    "        return value, critic\n",
    "    \n",
    "osar = ModelWrapper(q_net, fc_layer_params)\n",
    "osar.compile(loss=tf.keras.losses.Huber,\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "             run_eagerly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "num_hidden = 10\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 300#195.0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000002682C7B2040>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"<ipython-input-23-b25345845e68>\", line 41, in run\n",
      "    break  File \"<ipython-input-24-9de8e568e88a>\", line 27, in train_step\n",
      "    return train_step(  File \"<ipython-input-19-74d2ab289d71>\", line 31, in train_step\n",
      "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  File \"<ipython-input-22-8f55b8c3953c>\", line 24, in run_episode\n",
      "    action_logits_t, value = model(state, reward)  File \"C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 247, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000002682C7B2CD0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"<ipython-input-23-b25345845e68>\", line 41, in run\n",
      "    break  File \"<ipython-input-24-9de8e568e88a>\", line 27, in train_step\n",
      "    return train_step(  File \"<ipython-input-19-74d2ab289d71>\", line 31, in train_step\n",
      "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  File \"<ipython-input-22-8f55b8c3953c>\", line 24, in run_episode\n",
      "    action_logits_t, value = model(state, reward)  File \"C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 247, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000002682C7B2400>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"<ipython-input-23-b25345845e68>\", line 41, in run\n",
      "    break  File \"<ipython-input-24-9de8e568e88a>\", line 27, in train_step\n",
      "    return train_step(  File \"<ipython-input-19-74d2ab289d71>\", line 31, in train_step\n",
      "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  File \"<ipython-input-22-8f55b8c3953c>\", line 24, in run_episode\n",
      "    action_logits_t, value = model(state, reward)  File \"C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 247, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000002682C631A00>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"<ipython-input-23-b25345845e68>\", line 41, in run\n",
      "    break  File \"<ipython-input-24-9de8e568e88a>\", line 27, in train_step\n",
      "    return train_step(  File \"<ipython-input-19-74d2ab289d71>\", line 31, in train_step\n",
      "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  File \"<ipython-input-22-8f55b8c3953c>\", line 24, in run_episode\n",
      "    action_logits_t, value = model(state, reward)  File \"C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 247, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorgen\\AppData\\Local\\Temp\\tmp880iyp7a.py:14: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  retval_ = ag__.and_((lambda : (ag__.ld(state) is not None)), (lambda : ag__.and_((lambda : (ag__.ld(state) is not ())), (lambda : (ag__.ld(state) is not [])))))\n",
      "  0%|                                                                                         | 0/1000 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-32-8de13ce87871>:29 train_step  *\n        max_steps_per_episode)\n    <ipython-input-19-74d2ab289d71>:13 train_step  *\n        action_probs, values, rewards, x_values = run_episode(\n    <ipython-input-22-8f55b8c3953c>:24 run_episode  *\n        action_logits_t, value = model(state, reward)\n    <ipython-input-32-8de13ce87871>:32 call  *\n        value, critic, self.network_state = self.network(inputs, reward=reward, )#network_state=self.network_state)\n    C:\\Anaconda3\\envs\\osar\\lib\\site-packages\\OSAR\\network.py:230 call  *\n        context = self._context_generator(\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1008 __call__  **\n        self._maybe_build(inputs)\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:271 wrapper\n        input_shape = convert_shapes(input_shape, to_tuples=True)\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:196 convert_shapes\n        return map_structure_with_atomic(_is_atomic_shape, _convert_shape,\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:135 map_structure_with_atomic\n        mapped_values = [\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:136 <listcomp>\n        map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:123 map_structure_with_atomic\n        return map_fn(nested)\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:193 _convert_shape\n        input_shape = tuple(input_shape.as_list())\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1190 as_list\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-b25345845e68>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(model, env, inits, max_episodes, max_steps_per_episode, reward_threshold, gamma)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             episode_reward, hist = model.train_step(\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 max_steps_per_episode)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3885\u001b[0m     \u001b[1;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m     \u001b[1;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-32-8de13ce87871>:29 train_step  *\n        max_steps_per_episode)\n    <ipython-input-19-74d2ab289d71>:13 train_step  *\n        action_probs, values, rewards, x_values = run_episode(\n    <ipython-input-22-8f55b8c3953c>:24 run_episode  *\n        action_logits_t, value = model(state, reward)\n    <ipython-input-32-8de13ce87871>:32 call  *\n        value, critic, self.network_state = self.network(inputs, reward=reward, )#network_state=self.network_state)\n    C:\\Anaconda3\\envs\\osar\\lib\\site-packages\\OSAR\\network.py:230 call  *\n        context = self._context_generator(\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1008 __call__  **\n        self._maybe_build(inputs)\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2710 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:271 wrapper\n        input_shape = convert_shapes(input_shape, to_tuples=True)\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:196 convert_shapes\n        return map_structure_with_atomic(_is_atomic_shape, _convert_shape,\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:135 map_structure_with_atomic\n        mapped_values = [\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:136 <listcomp>\n        map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:123 map_structure_with_atomic\n        return map_fn(nested)\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py:193 _convert_shape\n        input_shape = tuple(input_shape.as_list())\n    C:\\Users\\Jorgen\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1190 as_list\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\n\n    ValueError: as_list() is not defined on an unknown TensorShape.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# osar = get_OSAR(num_hidden=num_hidden,)\n",
    "osar\n",
    "with tf.device('/device:GPU:0'):\n",
    "    osar_hist = run(osar,\n",
    "                    train_env,\n",
    "                    num_hidden,\n",
    "                    max_episodes,\n",
    "                    max_steps_per_episode,\n",
    "                    reward_threshold,\n",
    "                    gamma,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# baseline = get_baseline()\n",
    "# with tf.device('/device:CPU:0'):\n",
    "#     baseline_hist = run(baseline, \n",
    "#                         num_hidden,\n",
    "#                         max_episodes,\n",
    "#                         max_steps_per_episode,\n",
    "#                         reward_threshold,\n",
    "#                         gamma,\n",
    "#                         m_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# gru = get_GRU()\n",
    "# with tf.device('/device:CPU:0'):\n",
    "#     gru_hist = run(gru, \n",
    "#                    num_hidden,\n",
    "#                    max_episodes,\n",
    "#                    max_steps_per_episode,\n",
    "#                    reward_threshold,\n",
    "#                    gamma,\n",
    "#                    m_type=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['episode_reward', 'episode_x', 'episode_loss', 'episode_actions', 'episode_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_running_reward(rewards):\n",
    "    running_reward = [0]\n",
    "    for r in rewards:\n",
    "        running_reward.append(r*0.01 + running_reward[-1]*.99)\n",
    "    return running_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-632dc933438b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_running_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseline_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'baseline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_running_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosar_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'osar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_running_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgru_hist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'GRU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True, figsize=(10, 10))\n",
    "\n",
    "ax[0, 0].plot(get_running_reward(baseline_hist[:, 0]), '--', alpha=0.7, label='baseline')\n",
    "ax[0, 0].plot(get_running_reward(osar_hist[:, 0]), label='osar')\n",
    "ax[0, 0].plot(get_running_reward(gru_hist[:, 0]), alpha=0.4, label='GRU')\n",
    "ax[0, 0].set_title('Накопленная награда')\n",
    "ax[0, 0].legend()\n",
    "ax[1, 0].plot(baseline_hist[:, 2], '--', alpha=0.7, label='baseline')\n",
    "ax[1, 0].plot(osar_hist[:, 2], label='osar')\n",
    "ax[1, 0].plot(gru_hist[:, 2], alpha=0.4, label='gru')\n",
    "ax[1, 0].set_title('Функция потерь')\n",
    "ax[1, 0].legend()\n",
    "ax[0, 1].plot(get_running_reward(baseline_hist[:, 4]), '--', alpha=0.7, label='Критик baseline')\n",
    "ax[0, 1].plot(get_running_reward(osar_hist[:, 4]), label='Критик osar')\n",
    "ax[0, 1].plot(get_running_reward(baseline_hist[:, 0]),'--', alpha=0.7, label='Награда baseline')\n",
    "ax[0, 1].plot(get_running_reward(osar_hist[:, 0]), label='Награда osar')\n",
    "ax[0, 1].plot(get_running_reward(gru_hist[:, 0]), alpha=0.4, label='Награда GRU')\n",
    "ax[0, 1].plot(get_running_reward(gru_hist[:, 4]), alpha=0.4, label='Критик GRU')\n",
    "ax[0, 1].set_title('Реальная награда и ожидаемая награды Критика')\n",
    "ax[0, 1].legend()\n",
    "ax[1, 1].plot(baseline_hist[:, 3], '--', alpha=0.7, label='baseline')\n",
    "ax[1, 1].plot(osar_hist[:, 3], label='osar')\n",
    "ax[1, 1].plot(gru_hist[:, 3], alpha=0.4, label='GRU')\n",
    "ax[1, 1].set_title('Функция уверенности в выбранном действии')\n",
    "ax[1, 1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(gru_hist)), baseline_hist[:len(gru_hist), 0], alpha=0.7, label='baseline')\n",
    "plt.bar(range(len(gru_hist)), gru_hist[:, 0], alpha=0.4,\n",
    "        bottom=baseline_hist[:len(gru_hist), 0], label='GRU')\n",
    "plt.bar(range(len(gru_hist)), osar_hist[:len(gru_hist), 0], alpha=0.7, label='osar',\n",
    "       bottom=baseline_hist[:len(gru_hist), 0]+osar_hist[:len(gru_hist), 0])\n",
    "plt.title('Награда за эпизод игры')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "import sys\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "from pyvirtualdisplay import Display\n",
    "# import tensorflow_docs.vis.embed as embed\n",
    "\n",
    "if sys.platform != 'win32':\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "\n",
    "\n",
    "def render_episode(env, model: tf.keras.Model, max_steps: int, m_type:int=1, inits:int=num_hidden,): \n",
    "    screen = env.render(mode='rgb_array')\n",
    "    im = Image.fromarray(screen)\n",
    "\n",
    "    images = [im]\n",
    "    \n",
    "    reward, loop_var = tf.zeros((1, 1), tf.float32), tf.zeros((1, inits), tf.float32)\n",
    "\n",
    "    state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    for i in range(1, max_steps + 1):\n",
    "        \n",
    "        if m_type == 1: \n",
    "            action_probs, _, loop_var = model(state)\n",
    "            action = tf.random.categorical(action_probs, env.action_space.n)[0, 0]\n",
    "        else:\n",
    "            action_probs, _, loop_var = model([state, reward, tf.nn.softmax(loop_var)])\n",
    "            action = tf.random.categorical(action_probs, env.action_space.n)[0, 0]\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = tf.constant(state, dtype=tf.float32)\n",
    "        reward = tf.constant(reward, dtype=tf.float32)\n",
    "\n",
    "        # Render screen every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            screen = env.render(mode='rgb_array')\n",
    "            images.append(Image.fromarray(screen))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untrained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "buggy_baseline = get_baseline()\n",
    "run(buggy_baseline, 10, 100, max_steps_per_episode, reward_threshold, gamma, m_type=1)\n",
    "images = render_episode(env, buggy_baseline, max_steps_per_episode)\n",
    "image_untrained_baseline_file = '../images/alien-baseline-untrained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_untrained_baseline_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_untrained_baseline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "buggy_gru = get_GRU(num_hidden=10,)\n",
    "run(buggy_gru, 10, 100, max_steps_per_episode, reward_threshold, gamma, m_type=1)\n",
    "images = render_episode(env, buggy_gru, max_steps_per_episode, m_type=1)\n",
    "image_untrained_gru_file = '../images/alien-gru-untrained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_untrained_gru_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_untrained_osar_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSAR (Context Genrator + GRU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "buggy_osar = get_OSAR(num_hidden=10,)\n",
    "run(buggy_osar, 10, 100, max_steps_per_episode, reward_threshold, gamma, m_type=2)\n",
    "images = render_episode(env, buggy_osar, max_steps_per_episode, m_type=2)\n",
    "image_untrained_osar_file = '../images/alien-osar-untrained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_untrained_osar_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_untrained_osar_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "images = render_episode(env, baseline, max_steps_per_episode)\n",
    "image_trained_baseline_file = '../images/alien-baseline-trained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_trained_baseline_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_trained_baseline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "images = render_episode(env, gru, max_steps_per_episode)\n",
    "image_trained_gru_file = '../images/alien-gru-trained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_trained_gru_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_trained_baseline_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSAR (Context Genrator + GRU Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GIF image\n",
    "images = render_episode(env, osar, max_steps_per_episode, m_type=2)\n",
    "image_trained_osar_file = '../images/alien-osar-trained-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_trained_osar_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "# embed.embed_file(image_trained_osar_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.4",
   "language": "python",
   "name": "tf2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
